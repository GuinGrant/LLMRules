{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYEXc8YhC1JR"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mga9kjWC6D_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model_name = \"unsloth/llama-2-7b-bnb-4bit\"\n",
        "#Select from follow\n",
        "#unsloth/mistral-7b-bnb-4bit\n",
        "#unsloth/llama-2-7b-bnb-4bit\n",
        "#unsloth/qwen2-7b-bnb-4bit\n",
        "#unsloth/qwen3-14b-bnb-4bit\n",
        "max_output_lines = 10\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "model.bfloat16()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1phei0D_KI5m"
      },
      "outputs": [],
      "source": [
        "# Initialize a counter for outputs\n",
        "output_counter = 1\n",
        "\n",
        "# Loop to continuously take user input and log input-output pairs\n",
        "while True:\n",
        "    input_text = input(\"Enter your prompt (or type 'exit' to quit): \")\n",
        "    if input_text.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=1000)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    lines = generated_text.split('\\n')[:max_output_lines]\n",
        "    limited_text = '\\n'.join(lines)\n",
        "\n",
        "    print(f\"Generated Output {output_counter}:\")\n",
        "    print(limited_text)\n",
        "    print(\"=\" * 50)  # Separator for readability\n",
        "\n",
        "    # Log input and output to a file\n",
        "    with open(\"conversation_log.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
        "        log_file.write(f\"Output {output_counter}:\\n\")\n",
        "        log_file.write(\"User Input:\\n\")\n",
        "        log_file.write(input_text + \"\\n\\n\")\n",
        "        log_file.write(\"Model Output:\\n\")\n",
        "        log_file.write(limited_text + \"\\n\")\n",
        "        log_file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "    output_counter += 1  # Increment counter\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
