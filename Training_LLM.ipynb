{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "FPYcdFNF-b7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR_w0NZk6DTR"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/qwen2-7b-bnb-4bit\",\n",
        "    #Select from follow\n",
        "    #unsloth/mistral-7b-bnb-4bit\n",
        "    #unsloth/llama-2-7b-bnb-4bit\n",
        "    #unsloth/qwen2-7b-bnb-4bit\n",
        "    #unsloth/qwen3-14b-bnb-4bit\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files={\"data\": \"nftables_alpaca_300.jsonl\"}, split=\"data\") #path to training data\n",
        "\n",
        "# Split into train/validation/test\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_val = dataset['train']\n",
        "test_set = dataset['test']\n",
        "\n",
        "# Now split train_val into train and validation (90/10 of 80%)\n",
        "train_val = train_val.train_test_split(test_size=0.1, seed=42)  # ~10% of 90% = 10% of total\n",
        "train_set = train_val['train']\n",
        "validation_set = train_val['test']\n",
        "\n",
        "print(f\"Train size: {len(train_set)}, Validation size: {len(validation_set)}, Test size: {len(test_set)}\")\n",
        "\n",
        "# Save test set to a JSONL file\n",
        "test_set.to_json(\"test_set.jsonl\", orient=\"records\", lines=True)\n",
        "\n",
        "# Format prompts\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ec5bhAi_7-4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting\n",
        "train_set = train_set.map(formatting_prompts_func, batched=True)\n",
        "validation_set = validation_set.map(formatting_prompts_func, batched=True)\n",
        "test_set = test_set.map(formatting_prompts_func, batched=True)\n"
      ],
      "metadata": {
        "id": "Z0ZAvKbH8ElJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PEFT model setup\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "aGg6kHFy93cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Trainer\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=validation_set,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Can make training 5x faster if you have short sequences\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=1000,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "nlbkqoQ2-EbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# Usloth requires login token for weight&bias.  Register from wandb.ai and copy login token.\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "OtytKMli-F8g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate() # Unsloth validate function with the validation data\n",
        "\n",
        "print(\"Validation Results:\", results)"
      ],
      "metadata": {
        "id": "deVHtw8MpSxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./my_model_gwen2_nftables\")"
      ],
      "metadata": {
        "id": "S5IxIuvL-HWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip for download\n",
        "!zip -r my_model_gwen2_nftables.zip my_model_gwen2_nftables"
      ],
      "metadata": {
        "id": "a2gIy061uIu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}